{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f053ebd2",
   "metadata": {},
   "source": [
    "# Autocalibration ideas based on current implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6392a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from collections.abc import Callable, Sequence, Iterator, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import hist\n",
    "\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "import scipy\n",
    "\n",
    "from lgdo import lh5\n",
    "from lgdo.lh5.exceptions import LH5DecodeError\n",
    "from legendmeta import LegendMetadata\n",
    "from dspeed.processors import get_multi_local_extrema\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "\n",
    "proj_dir = \"/mnt/atlas02/projects/legend/sipm_qc\"\n",
    "lmeta  = LegendMetadata(os.path.join(proj_dir, \"metadata/legend-metadata-schwarz\"))\n",
    "chmap = lmeta.channelmap(\"20250807T150028Z\")\n",
    "chmap_sipm = chmap.map(\"system\", unique=False).spms\n",
    "#requires recent legend-datasets\n",
    "raw_keys = chmap_sipm.map(\"analysis.usability\", unique=False).on.map(\"daq.rawid\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b39e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(proj_dir, \"data/tier/raw/phy/p15/r004_part\")\n",
    "dsp_dir = os.path.join(proj_dir, \"manual_dsp/generated/p15r004dsp_part\")\n",
    "orig_dsp_dir = os.path.join(proj_dir, \"data/tier/dsp/phy/p15/r004\")\n",
    "dsp_files = glob.glob(dsp_dir+\"/l200-*-tier_dsp.lh5\")\n",
    "dsp_files.sort()\n",
    "def gimme_raw_filename_from_dsp(dspfilename: str):\n",
    "    return dspfilename.replace(dsp_dir, raw_dir).replace(\"tier_dsp\", \"tier_raw\")\n",
    "def gimme_orig_dsp_filename(dspfilename: str):\n",
    "    # get the original dsp files so I get pulser info\n",
    "    return dspfilename.replace(dsp_dir, orig_dsp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bd5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nopulser_mask(orig_dsp_file: Sequence[str] | str) -> ak.Array:\n",
    "    trap_puls = lh5.read_as(f\"ch{chmap['PULS01'].daq.rawid}/dsp/trapTmax\", orig_dsp_file, \"ak\")\n",
    "    return trap_puls < 100\n",
    "\n",
    "#trap_puls = lh5.read_as(f\"ch{chmap['PULS01'].daq.rawid}/dsp/trapTmax\", f_dsp, \"np\")\n",
    "#selection = trap_puls < 100\n",
    "#idx_not_pulser = np.where(selection)[0]\n",
    "#idx_pulser = np.where(~selection)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_wfs(wfs, ax, num=10):\n",
    "    \n",
    "    if wfs.shape[0] < num:\n",
    "        num = wfs.shape[0]\n",
    "        \n",
    "    t = np.arange(0, wfs.shape[1]*0.016, 0.016)\n",
    "    \n",
    "    for i in range(num):\n",
    "        t = np.arange(0, len(wfs[i])*0.016, 0.016)\n",
    "        ax.plot(t, wfs[i])\n",
    "        \n",
    "    ax.set_xlabel(\"Time (µs)\")\n",
    "    ax.set_ylabel(\"ADC counts / sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1760b",
   "metadata": {},
   "source": [
    "# Check uncalibrated spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ef145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energies(dsp_file: Sequence[str] | str, keys: Iterator[int], chmap, *, \n",
    "                 orig_dsp_file: Sequence[str] | str | None = None):\n",
    "    \"\"\"if orig_dsp_file is given: remove the pulser based on get_nopulser_mask\"\"\"\n",
    "    keys: list[int] = list(keys) # I need to access element 0 separately\n",
    "\n",
    "    def get_energy_object_name_function(dsp_file: str, raw_key: int, name_key: str) -> Callable[[int, str], str]:\n",
    "        if not os.path.isfile(dsp_file):\n",
    "            raise RuntimeError(f\"ERROR: no file: {dsp_file}\")\n",
    "        fcns = [lambda rawid, name: f\"ch{rawid}/dsp/energy\", lambda rawid, name: f\"ch{rawid}/dsp/energies\",\n",
    "                lambda rawid, name: f\"{name}/dsp/energy\", lambda rawid, name: f\"{name}/dsp/energies\"]\n",
    "        for fcn in fcns:\n",
    "            try:\n",
    "                _ = lh5.read_as(fcn(raw_key, name_key), dsp_file, \"ak\")\n",
    "                return fcn\n",
    "            except LH5DecodeError:\n",
    "                continue\n",
    "        raise RuntimeError(\"Have no clue how to extract energy info\")\n",
    "\n",
    "    energy_object_name_fcn = get_energy_object_name_function(dsp_file if isinstance(dsp_file, str) else dsp_file[0], keys[0], chmap.map(\"daq.rawid\")[keys[0]].name)\n",
    "    energies_dict = {}\n",
    "    #print(f\"{len(keys)} keys in dsp files\")\n",
    "    for ch in keys:\n",
    "        name = chmap.map(\"daq.rawid\")[ch].name\n",
    "        #energy = lh5.read_as(f\"{name}/dsp/energy\", f_dsp, \"ak\")\n",
    "        energy = lh5.read_as(energy_object_name_fcn(ch, name), dsp_file, \"ak\")\n",
    "        # remove pulser if we have original DSP files (containing pulser info)\n",
    "        # TODO perf: cache nopulser_mask\n",
    "        if orig_dsp_file is not None:\n",
    "            nopulser_mask = get_nopulser_mask(orig_dsp_file)\n",
    "            if len(nopulser_mask) < len(energy):\n",
    "                raise RuntimeError(\"Nopulser mask too short\")\n",
    "            elif len(nopulser_mask) > len(energy):\n",
    "                nopulser_mask = nopulser_mask[:len(energy)]\n",
    "            energy = energy[nopulser_mask]\n",
    "\n",
    "        energies = np.array(ak.flatten(energy))\n",
    "\n",
    "        energies_dict[name] = energies\n",
    "        \n",
    "    energies_dict = dict(sorted(energies_dict.items()))\n",
    "    \n",
    "    return energies_dict\n",
    "\n",
    "\n",
    "def gen_hist_by_quantile(data, quantile=0.99, nbins=200):\n",
    "    bins = np.linspace(0, np.round(np.quantile(data, quantile)), nbins+1)\n",
    "    n, be = np.histogram(data, bins)\n",
    "    return n, be\n",
    "\n",
    "def gen_hist_by_range(data, range, nbins=200):\n",
    "    n, be = np.histogram(data, range=range, bins=nbins)\n",
    "    return n, be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86836a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_pe_spectra(energies_dict):\n",
    "    \n",
    "    fig, ax = plt.subplots(10, 6, figsize=(20,20))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, (name, data) in enumerate(energies_dict.items()):\n",
    "\n",
    "        n, be = gen_hist_by_quantile(data, 0.96)\n",
    "        ax[i].stairs(n, be)\n",
    "            \n",
    "        ax[i].set_yscale(\"log\")\n",
    "        ax[i].set_title(name, fontsize=10)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fdee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = get_energies(dsp_files, raw_keys, chmap, orig_dsp_file=[gimme_orig_dsp_filename(dsp) for dsp in dsp_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "ax.stairs(*gen_hist_by_range(get_energies(dsp_files, raw_keys, chmap)[\"S002\"], (0,50)))\n",
    "ax.stairs(*gen_hist_by_range(get_energies(list(map(gimme_orig_dsp_filename, dsp_files)), raw_keys, chmap)[\"S002\"], (0,50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7266fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_all_pe_spectra(get_energies(gimme_orig_dsp_filename(dsp_files[0]), list(raw_keys), chmap))\n",
    "#plot_all_pe_spectra(get_energies(list(map(gimme_orig_dsp_filename, dsp_files)), list(raw_keys), chmap))\n",
    "plot_all_pe_spectra(energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11163786",
   "metadata": {},
   "source": [
    "# Simple calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0db901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default inputs\n",
    "peakfinder_defaults = {\n",
    "    \"a_delta_min_in\": 5e-3,\n",
    "    \"a_delta_max_in\": 5e-3,\n",
    "    \"search_direction\": 3,\n",
    "    \"a_abs_min_in\": 1000,\n",
    "    \"a_abs_max_in\": 1e-4,\n",
    "    \"min_peak_dist\": 6,\n",
    "    \"peakdist_compare_margin\": 1,\n",
    "    \"strict\": True\n",
    "}\n",
    "    \n",
    "def find_pe_peaks_in_hist(n, be, params: Mapping[str, Any]) -> np.typing.NDArray[np.int_]:\n",
    "    n = np.array(n)\n",
    "    be = np.array(be)\n",
    "\n",
    "    # Outputs\n",
    "    vt_max_out = np.zeros(shape=len(n) - 1)\n",
    "    vt_min_out = np.zeros(shape=len(n) - 1)\n",
    "    n_max_out = 0\n",
    "    n_min_out = 0\n",
    "\n",
    "    # Call the function with updated parameters\n",
    "    get_multi_local_extrema(\n",
    "        n,\n",
    "        params[\"a_delta_max_in\"] * np.max(n),\n",
    "        params[\"a_delta_min_in\"] * np.max(n),\n",
    "        params[\"search_direction\"],\n",
    "        params[\"a_abs_max_in\"] * np.max(n),\n",
    "        params[\"a_abs_min_in\"] * np.max(n),\n",
    "        vt_max_out,\n",
    "        vt_min_out,\n",
    "        n_max_out,\n",
    "        n_min_out,\n",
    "    ) # type: ignore\n",
    "\n",
    "    peakpos_indices = vt_max_out[~np.isnan(vt_max_out)].astype(np.int_)\n",
    "    return peakpos_indices\n",
    "\n",
    "class ResultCheckError(ValueError):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "def check_and_improve_PE_peaks(\n",
    "        peakpos_indices: np.typing.NDArray[np.int_], \n",
    "        n: np.typing.NDArray[Any],\n",
    "        be: np.typing.NDArray[Any],\n",
    "        params: Mapping[str, Any]\n",
    "        ) -> np.typing.NDArray[np.int_]:\n",
    "\n",
    "    if params.get(\"double_peak\", False):\n",
    "        if len(peakpos_indices) < 3:\n",
    "            raise ResultCheckError(f\"Require at least 3 found peaks for double-peak SiPMs; found only {len(peakpos_indices)}.\")\n",
    "        mean_1_2 = (peakpos_indices[1] + peakpos_indices[2]) // 2 # small bias but ok\n",
    "        peakpos_indices = np.concatenate((np.array([peakpos_indices[0], mean_1_2], dtype=np.int_), peakpos_indices[3:]))\n",
    "\n",
    "    min_peak_dist = params[\"min_peak_dist\"]\n",
    "    if params[\"strict\"]:\n",
    "        if len(peakpos_indices) < 2:\n",
    "            raise ResultCheckError(f\"Only {len(peakpos_indices)} peaks found. Either noise peak or 1pe peak not found.\")\n",
    "\n",
    "        if n[peakpos_indices[1]] > n[peakpos_indices[0]]:\n",
    "            raise ResultCheckError(f\"1pe peak larger than noise peak.\")\n",
    "\n",
    "        if peakpos_indices[1] - peakpos_indices[0] < min_peak_dist:\n",
    "            raise ResultCheckError(f\"Noise peak and 1pe peak too close together (< {min_peak_dist} bins).\")\n",
    "\n",
    "        if len(peakpos_indices) > 2:\n",
    "            if peakpos_indices[2] - peakpos_indices[1] < min_peak_dist:\n",
    "                raise ResultCheckError(f\"1pe peak and 2pe peak too close together (< {min_peak_dist} bins).\")\n",
    "            if (peakpos_indices[2] - peakpos_indices[1] + params[\"peakdist_compare_margin\"]) < peakpos_indices[1] - peakpos_indices[0]:\n",
    "                raise ResultCheckError(f\"Distance between 1pe and 2pe smaller than 0pe and 1pe (outside peakdist_compare_margin of {params[\"peakdist_compare_margin\"]}).\")\n",
    "    else:\n",
    "        if len(peakpos_indices) > 2:\n",
    "            if peakpos_indices[2] - peakpos_indices[1] < min_peak_dist:\n",
    "                print(f\"1pe peak and 2pe peak too close together (< {min_peak_dist} bins). Removing '1pe' peak.\")\n",
    "                peakpos_indices = peakpos_indices[peakpos_indices != peakpos_indices[1]]\n",
    "    return  peakpos_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_calibration(energies, gen_hist_params: Mapping[str, Any], \n",
    "                       peakfinder_params: Mapping[str, Any],\n",
    "                       calibration_params: Mapping[str, Any], *, \n",
    "                       ax = None, verbosity = 0) -> dict[str, float]:\n",
    "    \"\"\"Generate histogram and perform a simple peakfinder-based calibration. \n",
    "    If an axis is provided: plot on that (otherwise don't plot)\n",
    "    Does this for 1 SiPM; i.e. energies has to be a 1-d array of energies\"\"\"\n",
    "    match gen_hist_params:\n",
    "        case {\"quantile\": quantile, \"nbins\": nbins}:\n",
    "            n, be = gen_hist_by_quantile(energies, quantile, nbins)\n",
    "        case {\"range\": r, \"nbins\": nbins}:\n",
    "            n, be = gen_hist_by_range(energies, r, nbins)\n",
    "        case _:\n",
    "            raise TypeError(\"gen_hist_params does not match valid histogram type\")\n",
    "    peakpos_indices = find_pe_peaks_in_hist(n, be, peakfinder_params)\n",
    "    failed_checks = False\n",
    "    try:\n",
    "        peakpos_indices = check_and_improve_PE_peaks(peakpos_indices, n, be, peakfinder_params)  # might raise in case of failures\n",
    "    except ResultCheckError:\n",
    "        failed_checks = True\n",
    "        peaks = be[peakpos_indices] # use old indices for peaks\n",
    "        raise # runs finally before raise\n",
    "    else: \n",
    "        peaks = be[peakpos_indices]\n",
    "\n",
    "        if len(peaks) > 2: # use 1PE and 2PE\n",
    "            gain = peaks[2] - peaks[1]\n",
    "            c = 1/gain\n",
    "            offset = 1 - peaks[1] * c # 1pe peak at 1\n",
    "        else: # fallback: use 0 PE and 1 PE\n",
    "            gain = peaks[1] - peaks[0]\n",
    "            c = 1/gain\n",
    "            offset = 1 - peaks[1] * c # 1pe peak at 1   \n",
    "        \n",
    "        return {\"slope\": c, \"offset\": offset} # runs finally before return\n",
    "    finally: # draw in any case (for debugging); but choose color\n",
    "         if ax is not None:\n",
    "            hist_color = \"red\" if failed_checks else \"blue\"\n",
    "            line_color = \"grey\" if failed_checks else \"red\"\n",
    "            # uncalibrated histogram and peaks\n",
    "            ax.stairs(n, be, color=hist_color)\n",
    "            for p in peaks: # type: ignore\n",
    "                ax.axvline(x=p, color=line_color, ls=\":\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f984d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_simple_calibration(energies_dict, \n",
    "                             gen_hist_defaults: dict[str, Any], \n",
    "                             peakfinder_defaults: dict[str, Any],\n",
    "                             calibration_defaults: dict[str, Any], *,\n",
    "                             gen_hist_overrides: dict[str, dict[str, Any]] = {}, \n",
    "                             peakfinder_overrides: dict[str, dict[str, Any]] = {},\n",
    "                             calibration_overrides: dict[str, dict[str, Any]] = {},\n",
    "                             draw = False, \n",
    "                             nodraw_axes = False, \n",
    "                             verbosity = 0\n",
    "                             ) -> dict[str, dict[str, float]]:\n",
    "    \"\"\"Performs simple_calibration for all channels present in energies_dict\"\"\"\n",
    "    ret = {}\n",
    "    if draw:\n",
    "        fig, ax = plt.subplots(10, 6, figsize=(20,20))\n",
    "        ax_iter = iter(ax.ravel())\n",
    "    nr_unsuccessful_calibs = 0\n",
    "    for name, energies in energies_dict.items():\n",
    "        if draw:\n",
    "            ax = next(ax_iter)\n",
    "            ax.set_yscale(\"log\")\n",
    "            if nodraw_axes:\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            ax.set_title(name, fontsize=10)\n",
    "        else:\n",
    "            ax = None\n",
    "        try:\n",
    "            calib_results = simple_calibration(\n",
    "                energies,\n",
    "                gen_hist_defaults | gen_hist_overrides.get(name, {}),\n",
    "                peakfinder_defaults | peakfinder_overrides.get(name, {}),\n",
    "                calibration_defaults | calibration_overrides.get(name, {}),\n",
    "                ax=ax,  verbosity=verbosity)\n",
    "            ret[name] = calib_results\n",
    "        except ResultCheckError as e:\n",
    "            print(f\"Calibration failed for {name}: {e}\")\n",
    "            ret[name] = {\"slope\": np.nan, \"offset\": np.nan}\n",
    "            nr_unsuccessful_calibs += 1\n",
    "    \n",
    "    if nr_unsuccessful_calibs > 0 and verbosity >= -1:\n",
    "        print(f\"WARNING: {nr_unsuccessful_calibs} calibrations failed!\")\n",
    "    elif verbosity >= 0:\n",
    "        print(\"Info: All calibrations successful! :)\")\n",
    "\n",
    "    if draw:\n",
    "        fig.tight_layout()\n",
    "        if nodraw_axes: # have to do this also for non-drawn plots\n",
    "            for ax in ax_iter:\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            fig.subplots_adjust(wspace=0) # , hspace=0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "simple_calibration(energies[\"S100\"], {\"quantile\": 0.99, \"nbins\": 200}, peakfinder_defaults,\n",
    "                   {}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3687aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "simple_calibration(energies[\"S002\"], {\"quantile\": 0.98, \"nbins\": 200}, peakfinder_defaults, {}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c35d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peakfinder_overrides={\n",
    "    \"S046\": {\"a_delta_min_in\": 8e-3, \"a_delta_max_in\": 8e-3,},\n",
    "    \"S015\": {\"a_delta_min_in\": 1e-2, \"a_delta_max_in\": 2e-2,},\n",
    "    \"S083\": {\"double_peak\": True},\n",
    "    \"S090\": {\"double_peak\": True},\n",
    "    \"S095\": {\"double_peak\": True},\n",
    "    \"S096\": {\"double_peak\": True, \"a_delta_min_in\": 1e-3, \"a_delta_max_in\": 1e-3,},\n",
    "    \"S098\": {\"double_peak\": True},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_calib_output = multi_simple_calibration(energies,  {\"quantile\": 0.98, \"nbins\": 200}, peakfinder_defaults, {}, peakfinder_overrides=peakfinder_overrides, draw=True, nodraw_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibrated_histograms(energies, calib_output, range: tuple[float, float], nbins:int):\n",
    "    ret: dict[str, dict[str, np.typing.NDArray[Any]]] = {}\n",
    "    for name, energy in energies.items():\n",
    "        if name not in calib_output:\n",
    "            continue\n",
    "        c = calib_output[name][\"slope\"]\n",
    "        offset = calib_output[name][\"offset\"]\n",
    "        if np.isnan(c) or np.isnan(offset):\n",
    "            continue\n",
    "        energy_calibrated = energy * c + offset\n",
    "        n, be = gen_hist_by_range(energy_calibrated, range, nbins)\n",
    "        ret[name] = {\"n\": n, \"be\": be}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22026a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_histos = get_calibrated_histograms(energies, simple_calib_output, (0, 5), 200)\n",
    "fig, ax = plt.subplots(10, 6, figsize=(20, 20))\n",
    "ax = ax.ravel()\n",
    "for i, (name, histo) in enumerate(calibrated_histos.items()):\n",
    "    ax[i].set_yscale('log')\n",
    "    ax[i].stairs(histo[\"n\"], histo[\"be\"])\n",
    "    ax[i].set_title(name)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameter:\n",
    "    def __init__(self, init: tuple[float,float,float] | float):\n",
    "        if isinstance(init, tuple):\n",
    "            self.init = init[0]\n",
    "            self.min = init[1]\n",
    "            self.max = init[2]\n",
    "        else:\n",
    "            self.init = init\n",
    "            self.min = -np.inf\n",
    "            self.max = np.inf\n",
    "        self.result: np.float64 = np.float64(np.nan)\n",
    "    def set_result(self, result):\n",
    "        self.result = result\n",
    "\n",
    "class ModelComponent(ABC):\n",
    "    def __init__(self, params: dict[str, ModelParameter]):\n",
    "        self.params = params\n",
    "    def nr_params(self) -> int:\n",
    "        return len(self.params)\n",
    "    @abstractmethod\n",
    "    def eval(self, x, params) -> np.float64:\n",
    "        pass\n",
    "    def set_result_params(self, params):\n",
    "        for res, par in zip(params, self.params.values()):\n",
    "            par.set_result(res)\n",
    "    def get_result_params(self) -> Sequence[np.float64]:\n",
    "        return [p.result for p in self.params.values()]\n",
    "        \n",
    "class Gauss(ModelComponent):\n",
    "    def __init__(self, mean, sigma, scale):\n",
    "        super().__init__({\n",
    "            \"mean\": ModelParameter(mean),\n",
    "            \"sigma\": ModelParameter(sigma),\n",
    "            \"scale\": ModelParameter(scale)\n",
    "        })\n",
    "    def eval(self, x, params):\n",
    "        return params[2] * scipy.stats.norm.pdf(x, loc=params[0], scale=params[1])\n",
    "    \n",
    "class ExpoDec(ModelComponent):\n",
    "    def __init__(self, lamb, scale):\n",
    "        super().__init__({\n",
    "            \"lamb\": ModelParameter(lamb),\n",
    "            \"scale\": ModelParameter(scale)\n",
    "        })\n",
    "    def eval(self, x, params):\n",
    "        return params[1]*np.exp(-1*params[0]*x)\n",
    "    \n",
    "class TwoHyperbole(ModelComponent):\n",
    "    def __init__(self, p0, p1, p2):\n",
    "        super().__init__({\n",
    "            \"p0\": ModelParameter(p0),\n",
    "            \"p1\": ModelParameter(p1),\n",
    "            \"p2\": ModelParameter(p2),\n",
    "        })\n",
    "    def eval(self, x, params):\n",
    "        return params[0] + params[1]/x + params[2]/(x*x)\n",
    "    \n",
    "class Linear(ModelComponent):\n",
    "    def __init__(self, p0, p1):\n",
    "        super().__init__({\n",
    "            \"p0\": ModelParameter(p0),\n",
    "            \"p1\": ModelParameter(p1),\n",
    "        })\n",
    "    def eval(self, x, params):\n",
    "        return params[0] + params[1]*x\n",
    "    \n",
    "class SumModel(ModelComponent):\n",
    "    def __init__(self, components: dict[str, ModelComponent]):\n",
    "        passed_params = {}\n",
    "        for m_name, model in components.items():\n",
    "            for p_name, param in model.params.items():\n",
    "                passed_params[m_name+\".\"+p_name] = param\n",
    "        super().__init__(passed_params)\n",
    "        self.components = components\n",
    "    def eval(self, x, params) -> np.float64:\n",
    "        curr = 0\n",
    "        ret = np.float64(0)\n",
    "        for comp in self.components.values():\n",
    "            ret += comp.eval(x, params[curr:curr+comp.nr_params()])\n",
    "            curr += comp.nr_params()\n",
    "        return ret\n",
    "\n",
    "def evaluate(model_component: ModelComponent, x, param_values: list[Any]):\n",
    "    if not isinstance(param_values, list):\n",
    "        raise ValueError(\"param_values has to be a list so it can be modified\")\n",
    "    ret = model_component.eval(x, param_values[:model_component.nr_params()])\n",
    "    del param_values[:model_component.nr_params()]\n",
    "    return ret\n",
    "\n",
    "def get_inits(model_components: list[ModelComponent]) -> list[float]:\n",
    "    ret = []\n",
    "    for mc in model_components:\n",
    "        for p in mc.params.values():\n",
    "            ret.append(p.init)\n",
    "    return ret\n",
    "\n",
    "def get_upper_bounds(model_components: list[ModelComponent]) -> list[float]:\n",
    "    ret = []\n",
    "    for mc in model_components:\n",
    "        for p in mc.params.values():\n",
    "            ret.append(p.max)\n",
    "    return ret\n",
    "\n",
    "def get_lower_bounds(model_components: list[ModelComponent]) -> list[float]:\n",
    "    ret = []\n",
    "    for mc in model_components:\n",
    "        for p in mc.params.values():\n",
    "            ret.append(p.min)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ef83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fittable:\n",
    "    def __init__(self, model: ModelComponent, fit_range: tuple[float, float]):\n",
    "        self.model = model\n",
    "        self.fit_range = fit_range\n",
    "    def fit(self, bin_weights, bin_centers): # raises RuntimeError if fit failed\n",
    "        fit_range_mask = (bin_centers >= self.fit_range[0]) & (bin_centers <= self.fit_range[1])\n",
    "        range_bin_weights = bin_weights[fit_range_mask]\n",
    "        range_bin_centers = bin_centers[fit_range_mask]\n",
    "        def model_fcn(x, *p):\n",
    "            params = list(p)\n",
    "            ret = evaluate(self.model, x, params)\n",
    "            assert len(params) == 0\n",
    "            return ret\n",
    "        return scipy.optimize.curve_fit(\n",
    "                model_fcn, range_bin_centers, range_bin_weights, p0=get_inits([self.model]), \n",
    "                bounds=(get_lower_bounds([self.model]), get_upper_bounds([self.model])))\n",
    "    def draw(self, ax, params, color):\n",
    "        xx = np.linspace(self.fit_range[0], self.fit_range[1], 1000)\n",
    "        ax.plot(xx, self.model.eval(xx, params), color=color)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fit_results(gausses: list[Gauss]) -> None:\n",
    "    gauss_means = [g.params[\"mean\"].result for g in gausses]\n",
    "    if len(gauss_means) < 2:\n",
    "        raise ResultCheckError(f\"Too little gausses {len(gauss_means)}\")\n",
    "    for i, mean in enumerate(gauss_means):\n",
    "        peak_expect = i+1\n",
    "        if mean < peak_expect-0.45 or mean > peak_expect+0.45:\n",
    "            raise ResultCheckError(f\"Mean of PE peak #{peak_expect} out of range: {mean}\")\n",
    "        if i > 0:\n",
    "            if abs(gauss_means[i] - gauss_means[i-1] - 1) > 0.2:\n",
    "                raise ResultCheckError(f\"Distance between mean of PE peaks {peak_expect-1},{peak_expect} too far off 1: {gauss_means[i] - gauss_means[i-1]}\")\n",
    "\n",
    "\n",
    "\n",
    "def advanced_calibration(\n",
    "        precalibrated_histo: dict[str, np.typing.NDArray[Any]],\n",
    "        params: Mapping[str, Any], *,\n",
    "        ax = None, nofit=False, verbosity = 0\n",
    "        ) -> dict[str, float]:\n",
    "    \n",
    "    n = precalibrated_histo[\"n\"]\n",
    "    be = precalibrated_histo[\"be\"]\n",
    "    be_mid = (be[:-1] + be[1:]) / 2\n",
    "    assert len(be_mid) == len(be) - 1\n",
    "\n",
    "    fit_range = (params.get(\"fit_range_begin\", 0.5), params.get(\"fit_range_end\", 3.5))\n",
    "    fit_range_mask = (be_mid >= fit_range[0]) & (be_mid <= fit_range[1])\n",
    "\n",
    "    max_in_range = np.max(n[fit_range_mask])\n",
    "\n",
    "    gauss1 = Gauss((1, 0.5, 1.5), 0.1, (max_in_range/5, 0, np.inf))\n",
    "    gauss2 = Gauss((2, 1.5, 2.5), 0.1, (max_in_range/5, 0, np.inf))\n",
    "    gauss3 = Gauss((3, 2.5, 3.5), 0.1, (max_in_range/10, 0, np.inf))\n",
    "    expodec = ExpoDec((2, 0, np.inf), (max_in_range/2, 0, np.inf))\n",
    "    linear = Linear((max_in_range/100, 0, np.inf), -10)\n",
    "    th = TwoHyperbole(max_in_range/2, 100, (0,-1,1))\n",
    "\n",
    "    use_combo = False\n",
    "    fittables: list[Fittable] = []\n",
    "    \n",
    "    if use_combo:\n",
    "        fittables.append(Fittable(SumModel({\n",
    "            \"gauss1\": gauss1, \"gauss2\": gauss2, \"gauss3\": gauss3, \"expodec\": expodec, \"linear\": linear\n",
    "            }), fit_range))\n",
    "    else:\n",
    "        fittables += [Fittable(gauss1, (0.85, 1.15)), Fittable(gauss2, (1.85, 2.15)), Fittable(gauss3, (2.85, 3.15))]\n",
    "\n",
    "    failure: str = \"\"\n",
    "    try:\n",
    "        try:\n",
    "            if nofit:\n",
    "                raise RuntimeError(\"No fit performed, as requested\")\n",
    "            for fi in fittables:\n",
    "                fitted_params, pcov = fi.fit(n, be_mid)\n",
    "                fi.model.set_result_params(fitted_params)\n",
    "        except RuntimeError as e:\n",
    "            for fi in fittables:\n",
    "                fi.model.set_result_params(get_inits([fi.model]))\n",
    "            failure = \"fit\"\n",
    "            raise ResultCheckError(e) from e\n",
    "        else:\n",
    "            try:\n",
    "                check_fit_results([gauss1, gauss2, gauss3])\n",
    "                pass\n",
    "            except ResultCheckError as e:\n",
    "                failure = \"check\"\n",
    "                raise\n",
    "    except ResultCheckError:\n",
    "        raise\n",
    "    else:\n",
    "        #TODO: do calibration in this case!\n",
    "        return {\"slope\": np.nan, \"offset\": np.nan}\n",
    "    finally: # runs in any case; exception or not\n",
    "        if ax is not None:\n",
    "            ax.stairs(n, be)\n",
    "            match failure:\n",
    "                case \"\":\n",
    "                    color=\"green\"\n",
    "                case \"fit\":\n",
    "                    color=\"red\"\n",
    "                case \"check\":\n",
    "                    color=\"orange\"\n",
    "            for fi in fittables:\n",
    "                fi.draw(ax, fi.model.get_result_params(), color)\n",
    "            ax.set_ylim(((np.min(n) if np.min(n) > 0 else 0.5)*0.9, np.max(n)*1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28720df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "advanced_calibration(calibrated_histos[\"S007\"], {\"fit_range_end\": 3.8}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_advanced_calibration(calibrated_histo_dict, \n",
    "                             calibration_defaults: dict[str, Any], \n",
    "                             calibration_overrides: dict[str, dict[str, Any]] = {},\n",
    "                             draw = False, \n",
    "                             nodraw_axes = False, \n",
    "                             verbosity = 0\n",
    "                             ) -> dict[str, dict[str, float]]:\n",
    "    \"\"\"Performs advanced_calibration for all channels present in calibrated_histo_dict\"\"\"\n",
    "    ret = {}\n",
    "    if draw:\n",
    "        fig, ax = plt.subplots(10, 6, figsize=(20,20))\n",
    "        ax_iter = iter(ax.ravel())\n",
    "    nr_unsuccessful_calibs = 0\n",
    "    for name, calibrated_histo in calibrated_histo_dict.items():\n",
    "        if draw:\n",
    "            ax = next(ax_iter)\n",
    "            ax.set_yscale(\"log\")\n",
    "            if nodraw_axes:\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            ax.set_title(name, fontsize=10)\n",
    "        else:\n",
    "            ax = None\n",
    "        try:\n",
    "            calib_results = advanced_calibration(\n",
    "                calibrated_histo,\n",
    "                calibration_defaults | calibration_overrides.get(name, {}),\n",
    "                ax=ax,  verbosity=verbosity)\n",
    "            ret[name] = calib_results\n",
    "        except ResultCheckError as e:\n",
    "            print(f\"Calibration failed for {name}: {e}\")\n",
    "            ret[name] = {\"slope\": np.nan, \"offset\": np.nan}\n",
    "            nr_unsuccessful_calibs += 1\n",
    "    \n",
    "    if nr_unsuccessful_calibs > 0 and verbosity >= -1:\n",
    "        print(f\"WARNING: {nr_unsuccessful_calibs} calibrations failed!\")\n",
    "    elif verbosity >= 0:\n",
    "        print(\"Info: All calibrations successful! :)\")\n",
    "\n",
    "    if draw:\n",
    "        fig.tight_layout()\n",
    "        if nodraw_axes: # have to do this also for non-drawn plots\n",
    "            for ax in ax_iter:\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            fig.subplots_adjust(wspace=0) # , hspace=0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd69f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_advanced_calibration(calibrated_histos, {\"fit_range_end\": 3.8}, draw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecab473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# energies list was introduced here as sorted list of name, data tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a95734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OLD BACKUP\n",
    "def advanced_calibration(\n",
    "        precalibrated_histo: dict[str, np.typing.NDArray[Any]],\n",
    "        params: Mapping[str, Any], *,\n",
    "        ax = None, nofit=False, verbosity = 0\n",
    "        ) -> dict[str, float]:\n",
    "    \n",
    "    n = precalibrated_histo[\"n\"]\n",
    "    be = precalibrated_histo[\"be\"]\n",
    "    be_mid = (be[:-1] + be[1:]) / 2\n",
    "    assert len(be_mid) == len(be) - 1\n",
    "\n",
    "    fit_range = (params.get(\"fit_range_begin\", 0.5), params.get(\"fit_range_end\", 3.5))\n",
    "    fit_range_mask = (be_mid >= fit_range[0]) & (be_mid <= fit_range[1])\n",
    "    range_n = n[fit_range_mask]\n",
    "    range_be_mid = be_mid[fit_range_mask]\n",
    "\n",
    "    gauss1 = Gauss((1, 0.5, 1.5), 0.1, (np.max(range_n)/5, 0, np.inf))\n",
    "    gauss2 = Gauss((2, 1.5, 2.5), 0.1, (np.max(range_n)/5, 0, np.inf))\n",
    "    gauss3 = Gauss((3, 2.5, 3.5), 0.1, (np.max(range_n)/10, 0, np.inf))\n",
    "    expodec = ExpoDec((2, 0, np.inf), (np.max(range_n)/2, 0, np.inf))\n",
    "    linear = Linear((np.max(range_n)/100, 0, np.inf), -10)\n",
    "    th = TwoHyperbole(np.max(range_n)/2, 100, (0,-1,1))\n",
    "    background = SumModel({\"expodec\": expodec, \"linear\": linear})\n",
    "    model_components = [gauss1, gauss2, gauss3, background]\n",
    "\n",
    "    def model(x, *p):\n",
    "        params = list(p)\n",
    "        ret = evaluate(gauss1, x, params)\n",
    "        ret += evaluate(gauss2, x, params)\n",
    "        ret += evaluate(gauss3, x, params)\n",
    "        #ret += evaluate(expodec, x, params)\n",
    "        #ret += evaluate(linear, x, params)\n",
    "        ret += evaluate(background, x, params)\n",
    "        if len(params) != 0:\n",
    "            raise ValueError(\"Model messed up\")\n",
    "        return ret\n",
    "\n",
    "    failure: str = \"\"\n",
    "    try:\n",
    "        try:\n",
    "            if nofit:\n",
    "                raise RuntimeError(\"No fit performed, as requested\")\n",
    "            fitted_params, pcov = scipy.optimize.curve_fit(\n",
    "                model, range_be_mid, range_n, p0=get_inits(model_components), \n",
    "                bounds=(get_lower_bounds(model_components), get_upper_bounds(model_components)))\n",
    "        except RuntimeError as e:\n",
    "            #print(f\"ERROR: Fit failed: {e}\")\n",
    "            fitted_params = get_inits(model_components)\n",
    "            failure = \"fit\"\n",
    "            raise ResultCheckError(e) from e\n",
    "        else:\n",
    "            try:\n",
    "                check_fit_results(fitted_params[:9])\n",
    "            except ResultCheckError as e:\n",
    "                #print(f\"ERROR: Fit result check failed: {e}\")\n",
    "                failure = \"check\"\n",
    "                raise\n",
    "    except ResultCheckError:\n",
    "        raise\n",
    "    else:\n",
    "        #TODO: do calibration in this case!\n",
    "        return {\"slope\": np.nan, \"offset\": np.nan}\n",
    "    finally: # runs in any case; exception or not\n",
    "        if verbosity >= 1:\n",
    "            print(*fitted_params)\n",
    "        if ax is not None:\n",
    "            ax.stairs(n, be)\n",
    "            xx = np.linspace(range_be_mid[0], range_be_mid[-1], 1000)\n",
    "            match failure:\n",
    "                case \"\":\n",
    "                    color=\"green\"\n",
    "                case \"fit\":\n",
    "                    color=\"red\"\n",
    "                case \"check\":\n",
    "                    color=\"orange\"\n",
    "            ax.plot(xx, model(xx, *fitted_params), color=color)\n",
    "            ax.set_ylim(((np.min(n) if np.min(n) > 0 else 0.5)*0.9, np.max(n)*1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5edec45",
   "metadata": {},
   "source": [
    "## Unrefactored code below here ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50914ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 6, figsize=(20,20))\n",
    "ax = ax.ravel()  # Flatten the 2D axes array for easier indexing\n",
    "\n",
    "for i, (name, data) in enumerate(energies_list):\n",
    "\n",
    "    kwargs = overwrites_genhist.get(name, {})\n",
    "    quantile = kwargs.get(\"quantile\", quantile)\n",
    "    kwargs.pop(\"quantile\", None)\n",
    "\n",
    "    n, be, bins = gen_hist(data, quantile=quantile, **kwargs)\n",
    "\n",
    "    # reset quantile value\n",
    "    quantile = quantile_all\n",
    "\n",
    "    kwargs = overwrites_peaksearch.get(name, {})\n",
    "    try:\n",
    "        peakpos_indices = find_pe_peaks_in_hist(n, be, defaults=defaults, **kwargs)\n",
    "    except ValueError as e:\n",
    "        print(f\"{name}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    peaks = be[peakpos_indices]\n",
    "    \n",
    "    # if 1pe peak has double peak structure, take the mean of the double peak as 1pe peak position\n",
    "    if name in double_1pe_chs:\n",
    "        mean_1_2 = np.mean(peaks[1:3])\n",
    "        peaks = np.array([peaks[0], mean_1_2, peaks[3]])\n",
    "\n",
    "    if len(peaks) > 2:\n",
    "        gain = peaks[2] - peaks[1]\n",
    "        c = 1/gain\n",
    "        offset = 1 - peaks[1] * c # 1pe peak at 1\n",
    "    else:\n",
    "        gain = peaks[1] - peaks[0]\n",
    "        c = 1/gain\n",
    "        offset = 1 - peaks[1] * c # 1pe peak at 1    \n",
    "        \n",
    "        \n",
    "    # save vals\n",
    "    if name not in out_dict:\n",
    "        out_dict[name] = {}\n",
    "        \n",
    "    out_dict[name][\"slope\"] = c\n",
    "    out_dict[name][\"offset\"] = offset\n",
    "    \n",
    "    data = c * data + offset\n",
    "\n",
    "    bins = np.linspace(0,4.5,100)\n",
    "    ax[i].hist(data, bins, histtype=\"step\", linewidth=2)\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    \n",
    "    for x in range(1,5):\n",
    "        ax[i].axvline(x=x, ls=\"--\", color=\"grey\")\n",
    "    ax[i].set_yscale(\"log\")\n",
    "    ax[i].set_title(name, fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(folder_path + \"/spectra_fom.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Helper to recursively convert NumPy scalars to Python scalars\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(i) for i in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.generic,)):  # catches np.float64, np.int64, etc.\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "clean_dict = convert_numpy(out_dict)\n",
    "\n",
    "with open(\"./r004_simple_cal_params.yaml\", \"w\") as yaml_file:\n",
    "    yaml.dump(clean_dict, yaml_file, default_flow_style=False, sort_keys=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5999a",
   "metadata": {},
   "source": [
    "# Accurate calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "\n",
    "from iminuit import Minuit\n",
    "from iminuit.cost import LeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "dsp_file = \"./l200-p15-r004-phy-20250807T150028Z-tier_dsp_sigma2_snr.lh5\"\n",
    "\n",
    "with open(\"./r004_simple_cal_params.yaml\", \"r\") as yaml_file:\n",
    "    params_dict = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab47325",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_list = []\n",
    "energies_dict = {}\n",
    "\n",
    "names = []\n",
    "\n",
    "for ch in keys:\n",
    "    energies = lh5.read_as(f\"ch{ch}/dsp/energy\", dsp_file, library=\"ak\")\n",
    "    energies = np.array(ak.flatten(energies))\n",
    "    \n",
    "    name = chmap.map(\"daq.rawid\")[ch].name\n",
    "    names.append(name)\n",
    "    \n",
    "    c = params_dict[name][\"slope\"]\n",
    "    offset = params_dict[name][\"offset\"]\n",
    "\n",
    "    data = c * energies + offset\n",
    "\n",
    "    energies_list.append((name, data))\n",
    "    energies_dict[name] = data\n",
    "\n",
    "energies_list.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_thresh = {name: 0.7 for name in names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_GMM_fit(amp_simple_cal, pe_range=(0.7, 4.5), n_components=12, pe_peaks_model={1:1,2:2,3:3,4:4}, pe_range_peak=0.4):\n",
    "\n",
    "    # apply range mask\n",
    "    mask = (amp_simple_cal >= pe_range[0]) & (amp_simple_cal <= pe_range[1])\n",
    "    amp = amp_simple_cal[mask]\n",
    "\n",
    "    dmat = np.reshape(amp, (len(amp), 1))\n",
    "    gm = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=50)\n",
    "    gm.fit(dmat)\n",
    "    \n",
    "    means = gm.means_.flatten()\n",
    "    std_devs = np.sqrt(gm.covariances_.flatten())\n",
    "    weights = gm.weights_\n",
    "    \n",
    "    pe_peak_mean = []\n",
    "    pe_peak_std = []\n",
    "    for PE in pe_peaks_model.values():\n",
    "        range_peak = (PE-pe_range_peak, PE+pe_range_peak)\n",
    "        mask = (means >= range_peak[0]) & (means <= range_peak[1])\n",
    "        \n",
    "        mean_PE = np.dot(means[mask], weights[mask])/np.sum(weights[mask])\n",
    "        pe_peak_mean.append(mean_PE)\n",
    "        \n",
    "        std_PE = np.dot(std_devs[mask], weights[mask])/np.sum(weights[mask])\n",
    "        pe_peak_std.append(std_PE)\n",
    "        \n",
    "    return gm, dmat, pe_peaks_model, pe_peak_mean, pe_peak_std\n",
    "\n",
    "\n",
    "def plot_GMM_fit(gm, dmat, pe_peak_mean, pe_peak_std, ax=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "    means = gm.means_.flatten()\n",
    "    std_devs = np.sqrt(gm.covariances_.flatten())\n",
    "    weights = gm.weights_\n",
    "        \n",
    "    x = np.linspace(np.min(dmat), np.max(dmat), 1000).reshape(-1, 1)\n",
    "\n",
    "    ax.hist(dmat, bins=100, density=True, alpha=0.6, color='gray', label=\"Data\")\n",
    "\n",
    "    # Plot each Gaussian component individually\n",
    "    for i in range(len(means)):\n",
    "        gaussian_curve = weights[i] * norm.pdf(x, means[i], std_devs[i])\n",
    "        ax.plot(x, gaussian_curve) #, label=f\"Gaussian {i+1}\\nMean: {means[i]:.2f}, Sigma: {std_devs[i]:.2f}, Weight: {weights[i]:.2f}\")\n",
    "    \n",
    "    for i, pe in enumerate(pe_peak_mean):\n",
    "        ax.axvline(pe)\n",
    "        ax.axvspan(xmin=pe-pe_peak_std[i], xmax=pe+pe_peak_std[i], alpha=0.4)\n",
    "        \n",
    "    # Plot the overall GMM fit\n",
    "    ax.plot(x, np.exp(gm.score_samples(x)), color='black', lw=2, label='Overall GMM')\n",
    "\n",
    "    ax.set_xlabel('amplitude simple cal')\n",
    "    ax.set_ylabel('density')\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "def line(x, α, β):\n",
    "    return α + x * β \n",
    "\n",
    "  \n",
    "def apply_calibration_line_fit(data_x, data_y, data_yerr):\n",
    "\n",
    "    least_squares = LeastSquares(data_x, data_y, data_yerr, line)\n",
    "    m = Minuit(least_squares, α=0, β=1)\n",
    "    m.migrad()\n",
    "    m.hesse() \n",
    "\n",
    "    β = m.values[\"β\"]\n",
    "    α = m.values[\"α\"]\n",
    "    \n",
    "    # reverse engineer slope and offset such that amp_actual = slope * amp_data + offset\n",
    "    slope = 1/β\n",
    "    offset = - α/β\n",
    "    \n",
    "    return m, slope, offset\n",
    "\n",
    "\n",
    "def plot_calibration_line_fit(data_x, data_y, data_yerr, m, ax=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "    data_x = np.array(data_x)\n",
    "       \n",
    "    ax.errorbar(data_x, data_y, data_yerr, fmt=\"ok\", label=\"data\")\n",
    "    ax.plot(data_x, line(data_x, *m.values), label=\"fit\")\n",
    "\n",
    "    # display legend with some fit info\n",
    "    fit_info = [\n",
    "        f\"$\\\\chi^2$/$n_\\\\mathrm{{dof}}$ = {m.fval:.1f} / {m.ndof:.0f} = {m.fmin.reduced_chi2:.1f}\",\n",
    "    ]\n",
    "    for p, v, e in zip(m.parameters, m.values, m.errors):\n",
    "        fit_info.append(f\"{p} = ${v:.3f} \\\\pm {e:.3f}$\")\n",
    "\n",
    "    ax.legend(title=\"\\n\".join(fit_info), frameon=False)\n",
    "    ax.set_xlabel(\"true p.e.\")\n",
    "    ax.set_ylabel(\"p.e. data after simple cal\")\n",
    "    \n",
    "    \n",
    "def apply_accurate_cal(amp_simple_cal, slope, offset):\n",
    "    \n",
    "    return slope * amp_simple_cal + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides_GMM = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 6, figsize=(20,20))\n",
    "ax = ax.ravel()\n",
    "\n",
    "fig2, ax2 = plt.subplots(10, 6, figsize=(20,20))\n",
    "ax2 = ax2.ravel()\n",
    "\n",
    "out_dict = {}\n",
    "\n",
    "for i, (name, amp_simple_cal) in tqdm(enumerate(energies_list), total=len(energies_list)):\n",
    "    \n",
    "    pe_peaks_model={1:1,2:2,3:3,4:4}\n",
    "    pe_range = (pe_thresh[name], 4.5)\n",
    "    pe_range_peak = 0.4\n",
    "    \n",
    "    # Update parameters from overrides if they exist\n",
    "    if name in overrides_GMM:\n",
    "        params = overrides_GMM[name]\n",
    "        pe_peaks_model = params.get(\"pe_peaks_model\", pe_peaks_model)\n",
    "        pe_range = params.get(\"pe_range\", pe_range)\n",
    "        pe_range_peak = params.get(\"pe_range_peak\", pe_range_peak)\n",
    "\n",
    "    # Pass the parameters to fit_GMM\n",
    "    gm, dmat, pe_peaks_model, pe_peak_mean, pe_peak_std = apply_GMM_fit(\n",
    "        amp_simple_cal, \n",
    "        pe_range=pe_range, \n",
    "        n_components=12,\n",
    "        pe_peaks_model=pe_peaks_model,\n",
    "        pe_range_peak=pe_range_peak\n",
    "    )\n",
    "    \n",
    "    plot_GMM_fit(gm, dmat, pe_peak_mean, pe_peak_std, ax=ax[i])\n",
    "    \n",
    "    data_x = np.array(list(pe_peaks_model.keys()))\n",
    "    m, slope, offset = apply_calibration_line_fit(data_x, pe_peak_mean, pe_peak_std)  \n",
    "    \n",
    "    amp_accurate_cal = apply_accurate_cal(amp_simple_cal, slope, offset)\n",
    "    \n",
    "    bins = np.linspace(0,4.5,100)\n",
    "    ax2[i].hist(amp_simple_cal, bins, histtype=\"step\", label=\"simple cal\")\n",
    "    ax2[i].hist(amp_accurate_cal, bins, histtype=\"step\", label=\"GMM cal\")\n",
    "    ax2[i].legend()\n",
    "    ax2[i].set_yscale(\"log\")\n",
    "    \n",
    "    ax[i].set_title(name, fontsize=10)\n",
    "    ax2[i].set_title(name, fontsize=10)\n",
    "    \n",
    "    # save vals\n",
    "    if name not in out_dict:\n",
    "        out_dict[name] = {}\n",
    "        \n",
    "    out_dict[name][\"slope\"] = slope\n",
    "    out_dict[name][\"offset\"] = offset\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e58160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SiPM QC Analysis",
   "language": "python",
   "name": "sipm_qc_ana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
